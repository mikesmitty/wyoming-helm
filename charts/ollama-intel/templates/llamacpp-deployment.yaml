{{- if .Values.llamacpp.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "ollama-intel.fullname" . }}-llamacpp
  labels:
    {{- include "ollama-intel.llamacpp.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.llamacpp.replicaCount }}
  selector:
    matchLabels:
      {{- include "ollama-intel.llamacpp.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "ollama-intel.llamacpp.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.podSecurityContext }}
      securityContext:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if and .Values.llamacpp.model.ollama (ne .Values.llamacpp.model.pullPolicy "Never") }}
      initContainers:
      - name: model-downloader
        image: python:3.12-slim
        imagePullPolicy: IfNotPresent
        command:
        - python3
        - -c
        - |
          {{ .Files.Get "scripts/download-ollama-model.py" | nindent 10 }}

          # Main execution
          import sys
          MODEL_NAME = "{{ .Values.llamacpp.model.ollama }}"
          PULL_POLICY = "{{ .Values.llamacpp.model.pullPolicy }}"
          MODELS_DIR = "/models"

          # Extract model base name (without tag)
          if ':' in MODEL_NAME:
              model_base, tag = MODEL_NAME.split(':', 1)
          else:
              model_base = MODEL_NAME
              tag = 'latest'

          # Check if model exists
          from pathlib import Path
          manifest_path = Path(MODELS_DIR) / "manifests" / "registry.ollama.ai" / "library" / model_base / tag
          model_exists = manifest_path.exists()

          # Download model based on policy
          if PULL_POLICY == "Always" or (PULL_POLICY == "IfNotPresent" and not model_exists):
              print(f"Downloading model: {MODEL_NAME}")
              download_model(MODEL_NAME, MODELS_DIR)
          else:
              print(f"Model {MODEL_NAME} already exists, skipping download")
        {{- with .Values.llamacpp.securityContext }}
        securityContext:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        volumeMounts:
        {{- if .Values.llamacpp.persistence.enabled }}
        - name: llamacpp-models
          mountPath: /models
        {{- end }}
      {{- end }}
      containers:
      - name: llamacpp
        image: "{{ .Values.llamacpp.image.repository }}:{{ .Values.llamacpp.image.tag }}"
        imagePullPolicy: {{ .Values.llamacpp.image.pullPolicy }}
        {{- if or .Values.llamacpp.model.ollama .Values.llamacpp.model.huggingface .Values.llamacpp.model.path }}
        args:
        {{- if .Values.llamacpp.model.huggingface }}
        - --hf-repo
        - {{ .Values.llamacpp.model.huggingface | quote }}
        {{- else if .Values.llamacpp.model.path }}
        - --model
        - {{ printf "/models/%s" .Values.llamacpp.model.path | quote }}
        {{- else if .Values.llamacpp.model.ollama }}
        - --model
        - {{ printf "/models/manifests/registry.ollama.ai/library/%s" (.Values.llamacpp.model.ollama | replace ":" "/") | quote }}
        {{- end }}
        - --ctx-size
        - {{ .Values.llamacpp.args.ctxSize | quote }}
        - --n-gpu-layers
        - {{ .Values.llamacpp.args.nGpuLayers | quote }}
        {{- range .Values.llamacpp.args.extra }}
        - {{ . | quote }}
        {{- end }}
        {{- end }}
        ports:
        - name: llamacpp
          containerPort: {{ .Values.llamacpp.service.port }}
          protocol: TCP
        env:
        {{- if .Values.llamacpp.model.huggingface }}
        - name: LLAMA_CACHE
          value: /models/.cache
        {{- end }}
        {{- range $key, $value := .Values.llamacpp.env }}
        - name: {{ $key }}
          value: {{ $value | quote }}
        {{- end }}
        {{- with .Values.llamacpp.extraEnv }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
        {{- if .Values.llamacpp.livenessProbe.enabled }}
        livenessProbe:
          httpGet:
            path: {{ .Values.llamacpp.livenessProbe.httpGet.path }}
            port: {{ .Values.llamacpp.livenessProbe.httpGet.port }}
          initialDelaySeconds: {{ .Values.llamacpp.livenessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.llamacpp.livenessProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.llamacpp.livenessProbe.timeoutSeconds }}
          failureThreshold: {{ .Values.llamacpp.livenessProbe.failureThreshold }}
        {{- end }}
        {{- if .Values.llamacpp.readinessProbe.enabled }}
        readinessProbe:
          httpGet:
            path: {{ .Values.llamacpp.readinessProbe.httpGet.path }}
            port: {{ .Values.llamacpp.readinessProbe.httpGet.port }}
          initialDelaySeconds: {{ .Values.llamacpp.readinessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.llamacpp.readinessProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.llamacpp.readinessProbe.timeoutSeconds }}
          failureThreshold: {{ .Values.llamacpp.readinessProbe.failureThreshold }}
        {{- end }}
        {{- with .Values.llamacpp.resources }}
        resources:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with .Values.llamacpp.securityContext }}
        securityContext:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        volumeMounts:
        {{- if .Values.llamacpp.persistence.enabled }}
        - name: llamacpp-models
          mountPath: /models
        {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      volumes:
      {{- if .Values.llamacpp.persistence.enabled }}
      - name: llamacpp-models
        persistentVolumeClaim:
          claimName: {{ .Values.llamacpp.persistence.existingClaim | default (printf "%s-llamacpp-models" (include "ollama-intel.fullname" .)) }}
      {{- end }}
{{- end }}
