{{- if .Values.llamacpp.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "ollama-intel.fullname" . }}-llamacpp
  labels:
    {{- include "ollama-intel.llamacpp.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.llamacpp.replicaCount }}
  selector:
    matchLabels:
      {{- include "ollama-intel.llamacpp.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "ollama-intel.llamacpp.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.podSecurityContext }}
      securityContext:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if and .Values.llamacpp.model.ollama (ne .Values.llamacpp.model.pullPolicy "Never") }}
      initContainers:
      - name: model-downloader
        image: python:3.12-slim
        imagePullPolicy: IfNotPresent
        command:
        - /bin/sh
        - -c
        - |
          set -e
          MODEL_NAME="{{ .Values.llamacpp.model.ollama }}"
          PULL_POLICY="{{ .Values.llamacpp.model.pullPolicy }}"
          MODELS_DIR="/models"

          # Extract model base name (without tag)
          MODEL_BASE=$(echo "$MODEL_NAME" | awk -F':' '{print $1}')

          # Check if model exists
          MODEL_EXISTS=false
          if [ -d "$MODELS_DIR/manifests/registry.ollama.ai/library/$MODEL_BASE" ]; then
            MODEL_EXISTS=true
          fi

          # Download model based on policy
          if [ "$PULL_POLICY" = "Always" ] || { [ "$PULL_POLICY" = "IfNotPresent" ] && [ "$MODEL_EXISTS" = "false" ]; }; then
            echo "Installing uv and ollama-downloader..."
            pip install --no-cache-dir uv

            echo "Downloading model: $MODEL_NAME"
            uv tool run ollama-downloader model-download "$MODEL_NAME"

            echo "Model downloaded successfully"
          else
            echo "Model already exists, skipping download"
          fi
        env:
        - name: OLLAMA_MODELS
          value: "/models"
        {{- with .Values.llamacpp.securityContext }}
        securityContext:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        volumeMounts:
        {{- if .Values.llamacpp.persistence.enabled }}
        - name: llamacpp-models
          mountPath: /models
        {{- end }}
      {{- end }}
      containers:
      - name: llamacpp
        image: "{{ .Values.llamacpp.image.repository }}:{{ .Values.llamacpp.image.tag }}"
        imagePullPolicy: {{ .Values.llamacpp.image.pullPolicy }}
        {{- if or .Values.llamacpp.model.ollama .Values.llamacpp.model.huggingface .Values.llamacpp.model.path }}
        args:
        {{- if .Values.llamacpp.model.huggingface }}
        - --hf-repo
        - {{ .Values.llamacpp.model.huggingface | quote }}
        {{- else if .Values.llamacpp.model.path }}
        - --model
        - {{ printf "/models/%s" .Values.llamacpp.model.path | quote }}
        {{- else if .Values.llamacpp.model.ollama }}
        - --model
        - {{ printf "/models/manifests/registry.ollama.ai/library/%s" (.Values.llamacpp.model.ollama | replace ":" "/") | quote }}
        {{- end }}
        - --ctx-size
        - {{ .Values.llamacpp.args.ctxSize | quote }}
        - --n-gpu-layers
        - {{ .Values.llamacpp.args.nGpuLayers | quote }}
        {{- range .Values.llamacpp.args.extra }}
        - {{ . | quote }}
        {{- end }}
        {{- end }}
        ports:
        - name: llamacpp
          containerPort: {{ .Values.llamacpp.service.port }}
          protocol: TCP
        env:
        {{- if .Values.llamacpp.model.huggingface }}
        - name: LLAMA_CACHE
          value: /models/.cache
        {{- end }}
        {{- range $key, $value := .Values.llamacpp.env }}
        - name: {{ $key }}
          value: {{ $value | quote }}
        {{- end }}
        {{- with .Values.llamacpp.extraEnv }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
        {{- if .Values.llamacpp.livenessProbe.enabled }}
        livenessProbe:
          httpGet:
            path: {{ .Values.llamacpp.livenessProbe.httpGet.path }}
            port: {{ .Values.llamacpp.livenessProbe.httpGet.port }}
          initialDelaySeconds: {{ .Values.llamacpp.livenessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.llamacpp.livenessProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.llamacpp.livenessProbe.timeoutSeconds }}
          failureThreshold: {{ .Values.llamacpp.livenessProbe.failureThreshold }}
        {{- end }}
        {{- if .Values.llamacpp.readinessProbe.enabled }}
        readinessProbe:
          httpGet:
            path: {{ .Values.llamacpp.readinessProbe.httpGet.path }}
            port: {{ .Values.llamacpp.readinessProbe.httpGet.port }}
          initialDelaySeconds: {{ .Values.llamacpp.readinessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.llamacpp.readinessProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.llamacpp.readinessProbe.timeoutSeconds }}
          failureThreshold: {{ .Values.llamacpp.readinessProbe.failureThreshold }}
        {{- end }}
        {{- with .Values.llamacpp.resources }}
        resources:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        {{- with .Values.llamacpp.securityContext }}
        securityContext:
          {{- toYaml . | nindent 10 }}
        {{- end }}
        volumeMounts:
        {{- if .Values.llamacpp.persistence.enabled }}
        - name: llamacpp-models
          mountPath: /models
        {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      volumes:
      {{- if .Values.llamacpp.persistence.enabled }}
      - name: llamacpp-models
        persistentVolumeClaim:
          claimName: {{ .Values.llamacpp.persistence.existingClaim | default (printf "%s-llamacpp-models" (include "ollama-intel.fullname" .)) }}
      {{- end }}
{{- end }}
