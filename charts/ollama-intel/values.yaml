# Ollama Intel GPU service configuration
ollama:
  replicaCount: 1

  # Deployment strategy
  strategy:
    type: Recreate

  image:
    repository: ghcr.io/mikesmitty/ollama-intel-gpu
    tag: ""
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 11434

  # Environment variables for Intel GPU optimization
  env:
    # OneAPI device selector - use level_zero:0 for first Intel GPU
    ONEAPI_DEVICE_SELECTOR: "level_zero:0"
    # IPEX-LLM context size
    IPEX_LLM_NUM_CTX: "16384"
    # Ollama bind address
    OLLAMA_HOST: "0.0.0.0:11434"
    # Disable keep-alive to prevent model loading issues
    OLLAMA_KEEP_ALIVE: "-1"

  # Additional environment variables
  extraEnv: []
  # Example:
  # extraEnv:
  #   - name: CUSTOM_VAR
  #     value: "custom_value"

  # Persistent storage for Ollama models
  persistence:
    enabled: true
    storageClass: ""
    accessMode: ReadWriteOnce
    size: 50Gi # Ollama models can be large
    mountPath: /app/.ollama
    existingClaim: ""

  # Resource limits and requests
  # For Intel GPU acceleration, add gpu.intel.com/i915 or gpu.intel.com/xe resource
  # This requires the Intel GPU Device Plugin to be installed on your cluster
  # See: https://github.com/intel/intel-device-plugins-for-kubernetes
  resources: {}
    # limits:
    #   cpu: 4000m
    #   memory: 16Gi
    #   gpu.intel.com/i915: 1
    #   # gpu.intel.com/xe: 1
    # requests:
    #   cpu: 2000m
    #   memory: 8Gi
    #   gpu.intel.com/i915: 1
    #   # gpu.intel.com/xe: 1

  # Security context for the container
  securityContext:
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    seccompProfile:
      type: RuntimeDefault

  # Liveness probe configuration
  livenessProbe:
    enabled: true
    httpGet:
      path: /
      port: ollama
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3

  # Readiness probe configuration
  readinessProbe:
    enabled: true
    httpGet:
      path: /
      port: ollama
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  # Node selector for pod assignment
  nodeSelector: {}
  # Example: Schedule on nodes with Intel GPU (requires NFD operator)
  # nodeSelector:
  #   intel.feature.node.kubernetes.io/gpu: "true"

  # Tolerations for pod assignment
  tolerations: []
  # Example:
  # tolerations:
  #   - key: "gpu"
  #     operator: "Equal"
  #     value: "intel"
  #     effect: "NoSchedule"

  # Affinity for pod assignment
  affinity: {}

  # Pod security context
  podSecurityContext:
    fsGroup: 1001
    fsGroupChangePolicy: "OnRootMismatch"

  # Image pull secrets
  imagePullSecrets: []

# Open WebUI configuration
webui:
  enabled: true
  replicaCount: 1

  # Deployment strategy
  strategy:
    type: Recreate

  image:
    repository: ghcr.io/open-webui/open-webui
    # renovate: datasource=github-releases depName=open-webui/open-webui
    tag: "0.6.34"
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 8080
    # External port mapping (like OLLAMA_WEBUI_PORT in docker-compose)
    # This will be used if you create an Ingress or NodePort service
    externalPort: 3000

  # Additional environment variables
  extraEnv: []
  # Example:
  # extraEnv:
  #   - name: CUSTOM_VAR
  #     value: "custom_value"

  # Persistent storage for WebUI data
  persistence:
    enabled: true
    storageClass: ""
    accessMode: ReadWriteOnce
    size: 2Gi
    mountPath: /app/backend/data
    existingClaim: ""

  # Resource limits and requests
  resources: {}
    # limits:
    #   cpu: 1000m
    #   memory: 2Gi
    # requests:
    #   cpu: 500m
    #   memory: 1Gi

  # Security context for the container
  securityContext:
    runAsUser: 0 # Open WebUI may need root
    runAsNonRoot: false

  # Liveness probe configuration
  livenessProbe:
    enabled: false
    httpGet:
      path: /
      port: webui
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3

  # Readiness probe configuration
  readinessProbe:
    enabled: true
    httpGet:
      path: /
      port: webui
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  # Ingress configuration for external access
  ingress:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - host: ollama-webui.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
    #  - secretName: ollama-webui-tls
    #    hosts:
    #      - ollama-webui.local

  # Node selector for pod assignment
  nodeSelector: {}

  # Tolerations for pod assignment
  tolerations: []

  # Affinity for pod assignment
  affinity: {}

  # Pod security context
  podSecurityContext:
    fsGroup: 1001
    fsGroupChangePolicy: "OnRootMismatch"

  # Image pull secrets
  imagePullSecrets: []

# Llama.cpp Intel GPU service configuration
# Alternative to Ollama, runs llama.cpp with Intel GPU support
llamacpp:
  enabled: false
  replicaCount: 1

  # Deployment strategy
  strategy:
    type: Recreate

  image:
    repository: ghcr.io/mikesmitty/llama-cpp-intel
    tag: server-intel-b6869
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 8080

  # Model configuration
  model:
    # Ollama model name (e.g., "llama3.2:3b", "qwen2.5:7b")
    # Downloaded via ollama-downloader in init container
    # Leave empty if using HuggingFace model instead
    ollama: ""

    # HuggingFace model (e.g., "bartowski/Llama-3.2-3B-Instruct-GGUF:Q8_0")
    # Downloaded directly by llama.cpp server using --hf flag
    # Leave empty if using Ollama model instead
    # Format: "repo/model:filename" or just "repo/model" for default file
    huggingface: ""

    # Local model path (relative to persistence.mountPath)
    # Use this if you pre-loaded a model file
    # Example: "my-model.gguf"
    path: ""

    # Pull policy for init container (only applies to ollama models)
    # Options: Always, IfNotPresent, Never
    pullPolicy: IfNotPresent

  # Server arguments for llama.cpp
  # These are passed as command-line arguments to llama-server
  args:
    # Context size (token limit)
    ctxSize: 2048
    # Number of GPU layers to offload (higher = more GPU usage, faster inference)
    # Set to -1 to offload all layers
    nGpuLayers: 40
    # Additional custom arguments as a list
    # Example: ["--threads", "4", "--batch-size", "512"]
    extra: []

  # Environment variables for Intel GPU optimization
  env:
    # OneAPI device selector - use level_zero:0 for first Intel GPU
    ONEAPI_DEVICE_SELECTOR: "level_zero:0"

  # Additional environment variables
  extraEnv: []
  # Example:
  # extraEnv:
  #   - name: CUSTOM_VAR
  #     value: "custom_value"

  # Persistent storage for llama.cpp models
  # Separate from Ollama to support newer models that Ollama for Intel may not yet support
  persistence:
    enabled: true
    storageClass: ""
    accessMode: ReadWriteOnce
    size: 50Gi # Models can be large
    mountPath: /models
    existingClaim: ""

  # Resource limits and requests
  # For Intel GPU acceleration, add gpu.intel.com/i915 or gpu.intel.com/xe resource
  # This requires the Intel GPU Device Plugin to be installed on your cluster
  # See: https://github.com/intel/intel-device-plugins-for-kubernetes
  resources: {}
    # limits:
    #   cpu: 4000m
    #   memory: 16Gi
    #   gpu.intel.com/i915: 1
    #   # gpu.intel.com/xe: 1
    # requests:
    #   cpu: 2000m
    #   memory: 8Gi
    #   gpu.intel.com/i915: 1
    #   # gpu.intel.com/xe: 1

  # Security context for the container
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    seccompProfile:
      type: RuntimeDefault

  # Pod security context
  podSecurityContext:
    fsGroup: 1000
    fsGroupChangePolicy: "OnRootMismatch"

  # Image pull secrets
  imagePullSecrets: []

  # Liveness probe configuration
  livenessProbe:
    enabled: false
    httpGet:
      path: /health
      port: llamacpp
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3

  # Readiness probe configuration
  readinessProbe:
    enabled: true
    httpGet:
      path: /health
      port: llamacpp
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  # Node selector for pod assignment
  nodeSelector: {}
  # Example: Schedule on nodes with Intel GPU (requires NFD operator)
  # nodeSelector:
  #   intel.feature.node.kubernetes.io/gpu: "true"

  # Tolerations for pod assignment
  tolerations: []
  # Example:
  # tolerations:
  #   - key: "gpu"
  #     operator: "Equal"
  #     value: "intel"
  #     effect: "NoSchedule"

  # Affinity for pod assignment
  affinity: {}

# Name overrides
nameOverride: ""
fullnameOverride: ""
